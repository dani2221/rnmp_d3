# Домашна 3 - РНМП
### utils.ipynb
Накратко е направена EDA и поделба на податочното множество на offline и online
### spark_ml_train.ipynb
првата spark апликација, каде се вчитува податочното множество и се тренираат 3 модели:
- random forest
- logistic regression
- svm

Притоа искористена е крос валидација со grid search на параметри со помош на ParamGridBuilder и CrossValidator од spark ml

Евалуацијата се прави со f1-score метрика, при што не се добиваат многу добри модели, го зачувувам најдобриот - random forest со f1 od 0.22

За моделите се дефинира pipeline од 3 чекори:
- VectorAssembler - ги спојува сите features во една колона
- VectorIndexer - Помага со енкодирањето на категориските features, при што ставив параметар maxCategories, што значи дека ако постојат повеќе од 4 вредности, ќе се земе колоната како нумеричка
- Самиот модел - за тренирање и предвидување

Целиот pipeline се зачувува за полено користење во inference.

### producer.py
Зема ред по ред од online.csv и го дава на kafka topic health_data

### spark_online.ipynb
Најпрвин се поврзува со кафка со зададените параметри за да се користи structured streaming. Тука е ставена schema на податоците кои ги очекува од кафка за да може да се парсираат од json.

Бидејќи е зачуван целиот pipeline и сите трансформации во него, не треба да се прават повторно трансформациите и доволно е само да се повика `.transform`

Одкако ќе извадиме prediction, со `.drop` се исфрлаат непотребните колони и се праќа резултатот на health_data_predicted topic на кафка.

### consumer.py
Едноставна скрипта која ги чита евентите од health_data_predicted
